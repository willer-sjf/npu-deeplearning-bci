{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择三秒以后／实验中后期的数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import load_data\n",
    "\n",
    "seed = 0\n",
    "utils.set_random_seed(seed)\n",
    "PATH = '~/Desktop/Development/Python/dataset/data_preprocessed_python/'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "pdata  = pd.read_csv(PATH + 'preprocessed_data_1d.csv')\n",
    "plabel = pd.read_csv(PATH + 'preprocessed_label_1d.csv')\n",
    "ndata = np.array(pdata)\n",
    "ndata = ndata.reshape(40*32, 40, 8064)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "def label_map(x, pos=0, classes=2):\n",
    "    if x[pos] >= 5:\n",
    "        return [1, 0]\n",
    "    else:\n",
    "        return [0, 1]\n",
    "    \n",
    "def get_dataloader(file_path, batch_size=64, test_size=0.2, task='classify'):\n",
    "    \n",
    "    pdata  = pd.read_csv(file_path + 'preprocessed_data_1d.csv')\n",
    "    plabel = pd.read_csv(file_path + 'preprocessed_label_1d.csv')\n",
    "    ndata = np.array(pdata)\n",
    "    ndata = ndata.reshape(40*32, 40, 8064)\n",
    "    if task == 'classify':\n",
    "        nlabel = np.array(plabel.apply(label_map, axis=1))\n",
    "    else:\n",
    "        nlabel = np.array(plabel)\n",
    "    train_data, test_data, train_label, test_label = train_test_split(ndata, nlabel, test_size=0.20)\n",
    "    train_set = MyDataset(train_data, train_label)\n",
    "    test_set  = MyDataset(test_data, test_label)\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_set , batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def boost_dataloader(data, label, batch_size=64):\n",
    "    \n",
    "    train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.20)\n",
    "    train_set = MyDataset(train_data, train_label)\n",
    "    test_set  = MyDataset(test_data, test_label)\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_set , batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, label, task='classify'):\n",
    "        self.data  = data\n",
    "        self.label = label\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.task = task\n",
    "        if task not in ['classify', 'regression']:\n",
    "            raise ValueError(\"UNDEFINED TASK\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.label.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data  = self.data[index]\n",
    "        label = self.label[index]\n",
    "        \n",
    "        data  = torch.FloatTensor(data).to(self.device)\n",
    "        if self.task == 'classify':\n",
    "            label = torch.LongTensor(label).to(self.device)\n",
    "        else:\n",
    "            label = torch.FloatTensor(label).to(self.device)\n",
    "        return data, label\n",
    "        \n",
    "#train_loader, test_loader = get_dataloader(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_map_plus(x, pos=0, classes=2):\n",
    "    if classes == 3:\n",
    "        if x[pos] >= 7:\n",
    "            return [1, 0, 0]\n",
    "        elif x[pos] >= 4:\n",
    "            return [0, 1, 0]\n",
    "        else:\n",
    "            return [0, 0, 1]\n",
    "    else:\n",
    "        if x[pos] >= 5:\n",
    "            return [1, 0]\n",
    "        else:\n",
    "            return [0, 1]\n",
    "nlabel = np.array(plabel.apply(label_map_plus, axis=1))\n",
    "train_loader, test_loader = boost_dataloader(ndata, nlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 0]), list([1, 0]), list([1, 0]), list([0, 1]),\n",
       "       list([1, 0]), list([1, 0]), list([1, 0]), list([1, 0]),\n",
       "       list([0, 1]), list([0, 1])], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlabel[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ResidualBlock1d(nn.Module):\n",
    "    def __init__(self,in_channel, out_channel, kernel_size=3, stride=1, padding=1,dilation=1):\n",
    "        super(ResidualBlock1d, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm1d(in_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channel, out_channel, kernel_size, stride, padding, dilation),\n",
    "            nn.BatchNorm1d(out_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channel,out_channel, kernel_size, stride, padding, dilation),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        out += x\n",
    "        return out\n",
    "\n",
    "class ResidualBlock2d(nn.Module):\n",
    "    def __init__(self,in_channel, out_channel, kernel_size=3, stride=1, padding=1,dilation=1):\n",
    "        super(ResidualBlock2d, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channel,out_channel, kernel_size, stride, padding, dilation, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        out += x\n",
    "        return out\n",
    "        \n",
    "class PureCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PureCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=40 , out_channels=128, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=4, stride=2)\n",
    "        \n",
    "        self.max_pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.max_pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.block1 = ResidualBlock1d(128, 128, 3, padding=1)\n",
    "        self.block2 = ResidualBlock1d(128, 128, 3, padding=1)\n",
    "        self.block3 = ResidualBlock1d(128, 128, 3, padding=1)\n",
    "        self.block4 = nn.Conv1d(in_channels=128, out_channels=8, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.fn1 = nn.Linear(1992, 1024)\n",
    "        self.fn2 = nn.Linear(1024, 2)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(40)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        self.drop3 = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = x[:, :32, 2048:]\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.max_pool1(x)\n",
    "        \n",
    "        x = self.bn2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        \n",
    "        x = x.view(-1, 1992)\n",
    "        x = F.relu(self.fn1(x))\n",
    "        x = F.softmax(self.fn2(x), -1)\n",
    "        return x\n",
    "    \n",
    "class ConvLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(40, 512, 1)\n",
    "        self.fn1 = nn.Linear(hidden_size, 512)\n",
    "        self.fn2 = nn.Linear(512, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        h_0 = torch.zeros(1, x, 512)\n",
    "        c_0 = torch.zeros(1, x, 512)\n",
    "        \n",
    "        out = self.lstm(x,(h_0, c_0))\n",
    "        out = F.relu(self.fn1(out))\n",
    "        out = self.fn2(out)\n",
    "        return out\n",
    "    \n",
    "        \n",
    "class SubConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channel, hidden_channel=64, out_channel=2):\n",
    "        super(SubConvNet, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channel, hidden_channel, kernel_size=4, stride=2)\n",
    "        self.block1 = ResidualBlock1d(hidden_channel, hidden_channel)\n",
    "        self.block2 = ResidualBlock1d(hidden_channel, hidden_channel)\n",
    "        self.conb = nn.Conv1d(hidden_channel, out_channel, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.conv(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.conb(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return x\n",
    "    \n",
    "class AttentionAwareNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionAwareNet, self).__init__()\n",
    "        self.q_ = nn.Linear()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        attn = F.softmax(score, -1)\n",
    "        out = torch.matmul(attn, x)\n",
    "        return out\n",
    "    \n",
    "    def attention_net(self, lstm_output):\n",
    "        #print(lstm_output.size()) = (squence_length, batch_size, hidden_size*layer_size)\n",
    "\n",
    "        output_reshape = torch.Tensor.reshape(lstm_output, [-1, self.hidden_size*self.layer_size])\n",
    "        #print(output_reshape.size()) = (squence_length * batch_size, hidden_size*layer_size)\n",
    "\n",
    "        attn_tanh = torch.tanh(torch.mm(output_reshape, self.w_omega))\n",
    "        #print(attn_tanh.size()) = (squence_length * batch_size, attention_size)\n",
    "\n",
    "        attn_hidden_layer = torch.mm(attn_tanh, torch.Tensor.reshape(self.u_omega, [-1, 1]))\n",
    "        #print(attn_hidden_layer.size()) = (squence_length * batch_size, 1)\n",
    "\n",
    "        exps = torch.Tensor.reshape(torch.exp(attn_hidden_layer), [-1, self.sequence_length])\n",
    "        #print(exps.size()) = (batch_size, squence_length)\n",
    "\n",
    "        alphas = exps / torch.Tensor.reshape(torch.sum(exps, 1), [-1, 1])\n",
    "        #print(alphas.size()) = (batch_size, squence_length)\n",
    "\n",
    "        alphas_reshape = torch.Tensor.reshape(alphas, [-1, self.sequence_length, 1])\n",
    "        #print(alphas_reshape.size()) = (batch_size, squence_length, 1)\n",
    "\n",
    "        state = lstm_output.permute(1, 0, 2)\n",
    "        #print(state.size()) = (batch_size, squence_length, hidden_size*layer_size)\n",
    "\n",
    "        attn_output = torch.sum(state * alphas_reshape, 1)\n",
    "        #print(attn_output.size()) = (batch_size, hidden_size*layer_size)\n",
    "\n",
    "        return attn_output\n",
    "    \n",
    "class AttentionNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 time_lens=63, \n",
    "                 input_size=126, \n",
    "                 hidden_size=256, \n",
    "                 output_size=2, \n",
    "                 layer_size=1, \n",
    "                 batch_size=64,\n",
    "                 bidirectional=False\n",
    "                ):\n",
    "        super(AttentionNet, self).__init__()\n",
    "        \n",
    "        self.time_lens = time_lens\n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_size  = layer_size\n",
    "        self.batch_size  = batch_size\n",
    "        self.device = torch.device('cuda')\n",
    "        \n",
    "        self.subconv = SubConvNet(in_channel=40)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, layer_size, bidirectional=bidirectional)\n",
    "        #self.attn = AttentionAwareNet()\n",
    "        self.maxpool = nn.MaxPool1d(time_lens)\n",
    "        self.fn = nn.Linear(hidden_size * layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.chunk(self.time_lens, 2)\n",
    "        x = torch.stack(x, 1)\n",
    "        x = x.reshape(self.batch_size * self.time_lens, 40, 128)\n",
    "        x = self.subconv(x)\n",
    "        \n",
    "        x = x.view(self.batch_size, self.time_lens, self.input_size)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        h_0 = torch.zeros(self.layer_size, self.batch_size, self.hidden_size).to(self.device)\n",
    "        c_0 = torch.zeros(self.layer_size, self.batch_size, self.hidden_size).to(self.device)\n",
    "        \n",
    "        x, (h_final, c_final) = self.lstm(x, (h_0, c_0))\n",
    "        x = x.permute(1, 2, 0)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(self.batch_size, -1)\n",
    "        x = F.softmax(self.fn(x), -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class AttentionFeatureAwareNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                hidden_size,\n",
    "                out_size):\n",
    "        \n",
    "        self.cnet = SubConvNet()\n",
    "        self.attn = AttentionAwareNet()\n",
    "        self.fn = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(self.batch_size * self.channel_size, -1)\n",
    "        x = self.cnet(x)\n",
    "        x = x.view(self.batch_size, self.channel_size, -1)\n",
    "        x, score = self.attn(x)\n",
    "        x = self.fn(x)\n",
    "        return x, score\n",
    "        \n",
    "class bilstm_attn(torch.nn.Module):\n",
    "    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embed_dim, bidirectional, dropout, use_cuda, attention_size, sequence_length):\n",
    "        super(bilstm_attn, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "        self.use_cuda = use_cuda\n",
    "        self.sequence_length = sequence_length\n",
    "        self.lookup_table = nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=const.PAD)\n",
    "        self.lookup_table.weight.data.uniform_(-1., 1.)\n",
    "\n",
    "        self.layer_size = 1\n",
    "        self.lstm = nn.LSTM(self.embed_dim,\n",
    "                            self.hidden_size,\n",
    "                            self.layer_size,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            self.layer_size = self.layer_size * 2\n",
    "        else:\n",
    "            self.layer_size = self.layer_size\n",
    "\n",
    "        self.attention_size = attention_size\n",
    "        if self.use_cuda:\n",
    "            self.w_omega = Variable(torch.zeros(self.hidden_size * self.layer_size, self.attention_size).cuda())\n",
    "            self.u_omega = Variable(torch.zeros(self.attention_size).cuda())\n",
    "        else:\n",
    "            self.w_omega = Variable(torch.zeros(self.hidden_size * self.layer_size, self.attention_size))\n",
    "            self.u_omega = Variable(torch.zeros(self.attention_size))\n",
    "\n",
    "        self.label = nn.Linear(hidden_size * self.layer_size, output_size)\n",
    "\n",
    "    # self.attn_fc_layer = nn.Linear()\n",
    "\n",
    "    def attention_net(self, lstm_output):\n",
    "        #print(lstm_output.size()) = (squence_length, batch_size, hidden_size*layer_size)\n",
    "\n",
    "        output_reshape = torch.Tensor.reshape(lstm_output, [-1, self.hidden_size*self.layer_size])\n",
    "        #print(output_reshape.size()) = (squence_length * batch_size, hidden_size*layer_size)\n",
    "\n",
    "        attn_tanh = torch.tanh(torch.mm(output_reshape, self.w_omega))\n",
    "        #print(attn_tanh.size()) = (squence_length * batch_size, attention_size)\n",
    "\n",
    "        attn_hidden_layer = torch.mm(attn_tanh, torch.Tensor.reshape(self.u_omega, [-1, 1]))\n",
    "        #print(attn_hidden_layer.size()) = (squence_length * batch_size, 1)\n",
    "\n",
    "        exps = torch.Tensor.reshape(torch.exp(attn_hidden_layer), [-1, self.sequence_length])\n",
    "        #print(exps.size()) = (batch_size, squence_length)\n",
    "\n",
    "        alphas = exps / torch.Tensor.reshape(torch.sum(exps, 1), [-1, 1])\n",
    "        #print(alphas.size()) = (batch_size, squence_length)\n",
    "\n",
    "        alphas_reshape = torch.Tensor.reshape(alphas, [-1, self.sequence_length, 1])\n",
    "        #print(alphas_reshape.size()) = (batch_size, squence_length, 1)\n",
    "\n",
    "        state = lstm_output.permute(1, 0, 2)\n",
    "        #print(state.size()) = (batch_size, squence_length, hidden_size*layer_size)\n",
    "\n",
    "        attn_output = torch.sum(state * alphas_reshape, 1)\n",
    "        #print(attn_output.size()) = (batch_size, hidden_size*layer_size)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "    def forward(self, input_sentences, batch_size=None):\n",
    "        input = self.lookup_table(input_sentences)\n",
    "        input = input.permute(1, 0, 2)\n",
    "\n",
    "        if self.use_cuda:\n",
    "            h_0 = Variable(torch.zeros(self.layer_size, self.batch_size, self.hidden_size).cuda())\n",
    "            c_0 = Variable(torch.zeros(self.layer_size, self.batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(self.layer_size, self.batch_size, self.hidden_size))\n",
    "            c_0 = Variable(torch.zeros(self.layer_size, self.batch_size, self.hidden_size))\n",
    "\n",
    "        lstm_output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "        attn_output = self.attention_net(lstm_output)\n",
    "        logits = self.label(attn_output)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class ScaledDotProductAttention(nn.Module): # 点乘\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask): # 实现注意力公式\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module): # 多头注意力\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "   \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn\n",
    "    \n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs # inputs : [batch_size, len_q, d_model]\n",
    "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        return nn.LayerNorm(d_model)(output + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 0\n",
      "e 1\n",
      "e 2\n",
      "e 3\n",
      "e 4\n",
      "e 5\n",
      "e 6\n",
      "e 7\n",
      "e 8\n",
      "e 9\n",
      "e 10\n",
      "e 11\n",
      "e 12\n",
      "e 13\n",
      "e 14\n",
      "e 15\n",
      "e 16\n",
      "e 17\n",
      "e 18\n",
      "e 19\n",
      "e 20\n",
      "e 21\n",
      "e 22\n",
      "e 23\n",
      "e 24\n",
      "e 25\n",
      "e 26\n",
      "e 27\n",
      "e 28\n",
      "e 29\n",
      "e 30\n",
      "e 31\n",
      "e 32\n",
      "e 33\n",
      "e 34\n",
      "e 35\n",
      "e 36\n",
      "e 37\n",
      "e 38\n",
      "e 39\n",
      "e 40\n",
      "e 41\n",
      "e 42\n",
      "e 43\n",
      "e 44\n",
      "e 45\n",
      "e 46\n",
      "e 47\n",
      "e 48\n",
      "e 49\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "#net = PureCNN().to(device)\n",
    "net = AttentionNet().to(device)\n",
    "criterion_mse = nn.MSELoss()\n",
    "criterion_cel = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-2)\n",
    "writer = SummaryWriter(\"runs/ConvLSTM_Maxpool-2-classify_\" + str(datetime.datetime.now()))\n",
    "\n",
    "for i in range(50):\n",
    "    train_correct = train_total = 0\n",
    "    test_correct  = test_total  = 0\n",
    "    train_loss = 0\n",
    "    \n",
    "    net.train()\n",
    "    for input, label in train_loader:\n",
    "        output = net(input)\n",
    "        prediction = torch.argmax(output, 1)\n",
    "        label = torch.argmax(label, 1)\n",
    "        \n",
    "        loss = criterion_cel(output, label)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        train_correct += (prediction == label).sum().float()\n",
    "        train_total += len(label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "    for input, label in test_loader:\n",
    "        output = net(input)\n",
    "        prediction = torch.argmax(output, 1)\n",
    "        label = torch.argmax(label, 1)\n",
    "        \n",
    "        test_correct += (prediction == label).sum().float()\n",
    "        test_total += len(label)\n",
    "        \n",
    "    print('e', i)\n",
    "    writer.add_scalar('loss', train_loss, i)\n",
    "    writer.add_scalar('accuracy/train', train_correct/train_total, i)\n",
    "    writer.add_scalar('accuracy/test', test_correct /test_total, i)\n",
    "#     print(\"=\"*20)\n",
    "#     print(\"epoch:\",i)\n",
    "#     print(\" train acc:{:.4f}, loss:{:.4f}\".format(train_correct/train_total, train_loss))\n",
    "#     print(\" test  acc:{:.4f}\".format(test_correct /test_total))\n",
    "writer.close()\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net = SubConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 126])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = torch.randn(4, 40, 128)\n",
    "out = test_net(val)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5136, 0.4864],\n",
       "        [0.5034, 0.4966],\n",
       "        [0.5104, 0.4896],\n",
       "        [0.5109, 0.4891]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
