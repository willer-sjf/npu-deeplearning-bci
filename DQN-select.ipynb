{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "import random \n",
    "import copy \n",
    "import gc\n",
    "import sys\n",
    "sys.path.append(\"rl_method\")\n",
    "import env\n",
    "import agent\n",
    "import preprocess\n",
    "\n",
    "import load_data\n",
    "import utils\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/willer/Desktop/Development/Python/MyRepo/npu-deeplearning-bci/model/PretrainNet_T1.pkl'\n",
    "enet = preprocess.EncodeNet_T()\n",
    "pnet = preprocess.PretrainNet_T()\n",
    "pnet.load_state_dict(torch.load(model_path))\n",
    "\n",
    "enet_dict = enet.state_dict()\n",
    "for (name, param) in enet_dict.items():\n",
    "    enet_dict[name] = copy.deepcopy(pnet.state_dict()[name])\n",
    "enet.load_state_dict(enet_dict)\n",
    "enet.eval()\n",
    "\n",
    "n_classes = 2\n",
    "ndata, nlabel = load_data.get_grazdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlabel = nlabel.reshape(-1, 1)\n",
    "train_loader, test_loader = load_data.boost_dataloader(ndata, nlabel, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48416, 10, 128) (48416, 2)\n"
     ]
    }
   ],
   "source": [
    "enet.to(torch.device('cuda'))\n",
    "ndata = None\n",
    "nlabel = None\n",
    "with torch.no_grad():\n",
    "    for input, label in train_loader:\n",
    "        output = enet(input).cpu().numpy()\n",
    "        label = label.cpu().numpy().reshape(-1)\n",
    "        vec_label = np.eye(n_classes)[label]\n",
    "        \n",
    "        if str(type(ndata)) == \"<class 'NoneType'>\":\n",
    "            ndata  = output\n",
    "            nlabel = vec_label\n",
    "        else:\n",
    "            ndata  = np.concatenate([ndata, output], 0)\n",
    "            nlabel = np.concatenate([nlabel, vec_label], 0)\n",
    "\n",
    "    for input, label in test_loader:\n",
    "        output = enet(input).cpu().numpy()\n",
    "        label = label.cpu().numpy().reshape(-1)\n",
    "        vec_label = np.eye(n_classes)[label]\n",
    "\n",
    "        ndata  = np.concatenate([ndata, output], 0)\n",
    "        nlabel = np.concatenate([nlabel, vec_label], 0)\n",
    "\n",
    "print(ndata.shape, nlabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('rl_method/encode_data/encode_data_tem1.npy', ndata)\n",
    "np.save('rl_method/encode_data/encode_label_tem1.npy', nlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = np.load('rl_method/encode_data/encode_data_tem1.npy')\n",
    "nlabel = np.load('rl_method/encode_data/encode_label_tem1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_reward_net(input_size, model_path):\n",
    "\n",
    "    rnet = RewardNet(input_size)\n",
    "    pnet = PretrainNet_T()\n",
    "    pnet.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    rnet_dict = rnet.state_dict()\n",
    "    for (name, param)  in rnet_dict.items():\n",
    "        rnet_dict[name] = copy.deepcopy(pnet.state_dict()[name])\n",
    "    rnet.load_state_dict(rnet_dict)\n",
    "    rnet.eval()\n",
    "    return rnet\n",
    "\n",
    "class PretrainNet_T(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel=3,\n",
    "        sequence_lens=1000,\n",
    "        time_lens=10,\n",
    "        hidden_size=64,\n",
    "        output_size=2,\n",
    "        layer_size=1,\n",
    "        bidirectional=True\n",
    "    ):\n",
    "        super(PretrainNet_T, self).__init__()\n",
    "\n",
    "        if sequence_lens % time_lens != 0:\n",
    "            raise ValueError(\"Invalid time lens\")\n",
    "\n",
    "        self.in_channel  = in_channel\n",
    "        self.time_lens   = time_lens\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_size  = layer_size\n",
    "        self.window_size = sequence_lens // time_lens\n",
    "        self.device      = torch.device('cuda')\n",
    "\n",
    "        self.subconv    = SubConvNet(in_channel=in_channel, out_channel=4)\n",
    "        self.input_size = self._adaptive_feature_size()\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_size, hidden_size, layer_size, bidirectional=bidirectional)\n",
    "        if bidirectional:\n",
    "            self.layer_size *= 2\n",
    "\n",
    "        self.fn1  = nn.Linear(hidden_size * self.layer_size, 128)\n",
    "        self.fn2  = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = x.chunk(self.time_lens, 2)\n",
    "        x = torch.stack(x, 1)\n",
    "        x = x.reshape(batch_size * self.time_lens, self.in_channel, self.window_size)\n",
    "\n",
    "        x = self.subconv(x)\n",
    "        x = x.view(batch_size, self.time_lens, self.input_size)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        h_0 = torch.zeros(self.layer_size, batch_size, self.hidden_size).to(self.device)\n",
    "        c_0 = torch.zeros(self.layer_size, batch_size, self.hidden_size).to(self.device)\n",
    "        x, (h_final, c_final) = self.lstm(x, (h_0, c_0))\n",
    "        # seq, batch, feature\n",
    "        x = x.permute(1, 2, 0)\n",
    "\n",
    "        x = F.avg_pool1d(x, self.time_lens)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = F.relu(self.fn1(x), inplace=True)\n",
    "        x = F.softmax(self.fn2(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "    def _adaptive_feature_size(self):\n",
    "        x = torch.zeros(1, self.in_channel, self.window_size)\n",
    "        return self.subconv(x).view(-1).shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "def calculate_reward(vector_a, vector_b):\n",
    "    return -np.sum(np.abs(vector_a - vector_b))\n",
    "\n",
    "def cal_cosine_similarity(vector_a, vector_b):\n",
    "    inner = np.dot(vector_a, vector_b.transpose())\n",
    "    norm = np.linalg.norm(vector_a) * np.linalg.norm(vector_b)\n",
    "    return inner / norm\n",
    "\n",
    "class FeatureManager:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.size = data.shape[-1]\n",
    "        self.index = list(range(data.shape[0]))\n",
    "\n",
    "    def drop(self, action):\n",
    "        self.index.remove(self.index[action])\n",
    "\n",
    "    def state(self):\n",
    "        new_state = self.data[self.index]\n",
    "        new_size  = new_state.shape[0]\n",
    "        new_index = list(range(new_size))\n",
    "\n",
    "        avg_state = torch.mean(new_state, 0)\n",
    "        ret_state = []\n",
    "        for i in range(new_size):\n",
    "            remaining = new_state[new_index[:i] + new_index[i+1:]]\n",
    "            mean_state, var_state = self.mean_var_state(remaining)\n",
    "            ret_state.append([new_state[i], mean_state, var_state])\n",
    "        return avg_state, ret_state\n",
    "\n",
    "    def mean_var_state(self, remaining_feature):\n",
    "        shape = remaining_feature.shape[1]\n",
    "        size  = remaining_feature.shape[0]\n",
    "        mean_state = torch.mean(remaining_feature, 0)\n",
    "        var_state  = torch.mean(torch.pow(remaining_feature - mean_state, 2), 0)\n",
    "        return mean_state, var_state\n",
    "\n",
    "\n",
    "class DropEnv:\n",
    "    def __init__(self, tdata, vdata, tlabel, vlabel, drop_reward, reward_model_path):\n",
    "\n",
    "        self.drop_reward = drop_reward\n",
    "        self.random_stop = 0.8\n",
    "        self.training = True\n",
    "        \n",
    "        self.tdata  = torch.from_numpy(tdata)\n",
    "        self.vdata  = torch.from_numpy(vdata)\n",
    "        self.tlabel = tlabel\n",
    "        self.vlabel = vlabel\n",
    "        \n",
    "        self.tdata_size = self.tdata.shape[0]\n",
    "        self.vdata_size = self.vdata.shape[0]\n",
    "        self.channel_size = self.tdata.shape[1]\n",
    "        \n",
    "        self.reward_module = get_reward_net(tdata[0].shape[-1], reward_model_path)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.manager.drop(action)\n",
    "        avg_state, state = self.manager.state()\n",
    "        cls_vector = self.reward_module(avg_state).numpy()\n",
    "        self.new_sim = calculate_reward(cls_vector, self.current_label)\n",
    "        if self.training:\n",
    "            reward = self.new_sim - self.old_sim + self.drop_reward\n",
    "        else:\n",
    "            reward = self.new_sim - self.old_sim\n",
    "        self.old_sim = self.new_sim\n",
    "\n",
    "        done = False\n",
    "        if reward < 0 and (self.training == True and random.random() < self.random_stop \\\n",
    "                           or self.training == False):\n",
    "            done = True\n",
    "        \n",
    "        if self.training:\n",
    "            return state, reward, done\n",
    "        else:\n",
    "            res = np.argmax(cls_vector)\n",
    "            return state, float(res==self.current_num), done\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "\n",
    "    def reset(self):\n",
    "        if self.training:\n",
    "            index = random.randint(0, self.tdata_size-1)\n",
    "            single_data = self.tdata[index]\n",
    "            self.current_label = self.tlabel[index]\n",
    "        else:\n",
    "            index = random.randint(0, self.vdata_size-1)\n",
    "            single_data = self.vdata[index]\n",
    "            self.current_label = self.vlabel[index]\n",
    "            \n",
    "        self.current_num = np.argmax(self.current_label)\n",
    "        self.manager = FeatureManager(single_data)\n",
    "            \n",
    "        random_drop_num = 0\n",
    "        \n",
    "        if self.training:\n",
    "            random_drop_num = random.randint(1, self.channel_size//4)\n",
    "            random_drop_idx = random.sample(range(0, self.channel_size - random_drop_num - 1), random_drop_num)\n",
    "            for idx in random_drop_idx:\n",
    "                self.manager.drop(idx)\n",
    "\n",
    "        init_avg_state, init_state = self.manager.state()\n",
    "        init_cls_vector = self.reward_module(init_avg_state).numpy()\n",
    "        init_cls_num = np.argmax(init_cls_vector)\n",
    "        self.old_sim = calculate_reward(init_cls_vector, self.current_label)\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        if self.training:\n",
    "            return init_state, random_drop_num\n",
    "        else:\n",
    "            return init_state, float(init_cls_num==self.current_num), random_drop_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_shape(origin_reward, discount):\n",
    "    length = len(origin_reward)\n",
    "    new_reward = np.zeros_like(origin_reward, dtype=np.float32)\n",
    "    for i in reversed(range(length)):\n",
    "        new_reward[i] = origin_reward[i] + (discount * new_reward[i+1] if i+1 < length else 0)\n",
    "    return new_reward\n",
    "\n",
    "class Q_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, v_dim, h_dim=64):\n",
    "        super(Q_Net,self).__init__()\n",
    "\n",
    "        self.v_dim = v_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.pre_feature = nn.Sequential(\n",
    "            nn.Linear(v_dim, h_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.first_order_feature= nn.Sequential(\n",
    "            nn.Linear(v_dim, h_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.second_order_feature = nn.Sequential(\n",
    "            nn.Linear(v_dim, h_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(h_dim*3, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim  , 1)\n",
    "\n",
    "    def forward(self, feature, mean_feature=None, var_feature=None):\n",
    "\n",
    "        if mean_feature == None:\n",
    "            feature, mean_feature, var_feature = feature.chunk(3, -1)\n",
    "        f_pre = self.pre_feature(feature)\n",
    "        f_fst = self.first_order_feature(mean_feature - feature)\n",
    "        f_scd = self.second_order_feature(var_feature)\n",
    "\n",
    "        f_merge = torch.cat([f_pre, f_fst, f_scd], -1)\n",
    "        q = F.relu(self.fc1(f_merge))\n",
    "        q = self.fc2(q)\n",
    "        return q\n",
    "    \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "    \n",
    "        self._storage  = []\n",
    "        self._maxsize  = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, reward, obs_tp1, done):\n",
    "        data = (obs_t, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, rewards, obses_tp1, dones = [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, reward, obs_tp1, done = data\n",
    "            obses_t.append(obs_t)\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(obs_tp1,)\n",
    "            dones.append(done)\n",
    "        return obses_t, rewards, obses_tp1, dones\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "    \n",
    "    \n",
    "class ADAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        label,\n",
    "        reward_model_path,\n",
    "        gamma=0.98,\n",
    "        epsilon=0.3,\n",
    "        max_drop=9,\n",
    "        buffer_size=2000,\n",
    "        batch_size=512,\n",
    "        train_epoch=1000,\n",
    "        drop_reward=3e-3,\n",
    "    ):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.max_drop = max_drop\n",
    "        self.train_epoch = train_epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        tdata, vdata, tlabel, vlabel = train_test_split(data, label, test_size=0.2)\n",
    "        self.env = DropEnv(tdata, vdata, tlabel, vlabel, drop_reward, reward_model_path)\n",
    "        \n",
    "        self.device = torch.device('cuda')\n",
    "        self.eval_net = Q_Net(data.shape[-1])\n",
    "        self.eval_net_gpu = Q_Net(data.shape[-1]).to(self.device)\n",
    "        \n",
    "        self.val_net = get_reward_net(data[0].shape[-1], reward_model_path)\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        self.writer = SummaryWriter(\"adrl-runs/ADAgent_\" + str(datetime.datetime.now()))\n",
    "\n",
    "        self.interaction_counter = 0\n",
    "        self.validation_counter = 0\n",
    "        self.learn_step_counter = 0\n",
    "        self.step_update = 10\n",
    "        self.training = True\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=15, gamma=0.98)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        action = -1\n",
    "        q_max = None\n",
    "        action_dim = len(state) - 1\n",
    "        if random.random() < self.epsilon or not self.training:\n",
    "            for index, (fea, mean, var) in enumerate(state):\n",
    "                q = self.eval_net(fea, mean, var)\n",
    "                if action == -1:\n",
    "                    action = index\n",
    "                    q_max = q\n",
    "                elif q > q_max:\n",
    "                    action = index\n",
    "                    q_max = q\n",
    "        else:\n",
    "            action = random.randint(0, action_dim)\n",
    "            q_max  = self.eval_net(*state[action])\n",
    "        return torch.cat(state[action], 0), action, q_max\n",
    "    \n",
    "    def test(self, test_size=25):\n",
    "        self.validation_counter += 1\n",
    "        total_init_sim = 0\n",
    "        total_drop_sim = 0\n",
    "        total_drop_lens = 0\n",
    "        \n",
    "        i = 0\n",
    "        self.training = False\n",
    "        for _ in range(test_size):\n",
    "            state, init_sim, drop_size = self.env.reset()\n",
    "            for i in range(self.max_drop - drop_size):\n",
    "                state_tp1, action, q_pred = self.select_action(state)\n",
    "                new_state, result, done = self.env.step(action)\n",
    "                if done:\n",
    "                    break\n",
    "                state = new_state\n",
    "                \n",
    "            total_init_sim  += init_sim\n",
    "            total_drop_sim  += result\n",
    "            total_drop_lens += i\n",
    "            \n",
    "        self.training = True \n",
    "        self.writer.add_scalar('accuracy/raw', total_init_sim / test_size, self.validation_counter)\n",
    "        self.writer.add_scalar('accuracy/drop', total_drop_sim / test_size, self.validation_counter)\n",
    "        self.writer.add_scalar('accuracy/radio(drop_to_raw)', total_drop_sim / total_init_sim, self.validation_counter)\n",
    "        self.writer.add_scalar('drop_lens/val', total_drop_lens / test_size, self.validation_counter)\n",
    "            \n",
    "    def train(self):\n",
    "        for _ in range(self.train_epoch):\n",
    "            self.interaction_counter += 1\n",
    "            state, drop_size = self.env.reset()\n",
    "\n",
    "            i = 0\n",
    "            for i in range(self.max_drop - drop_size):\n",
    "                state_tp1, action, q_pred = self.select_action(state)\n",
    "                new_state, reward, done = self.env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                if i != 0:\n",
    "                    self.memory.add(state_t, reward, state_tp1, done)\n",
    "                state = new_state\n",
    "                state_t = state_tp1\n",
    "                \n",
    "            self.writer.add_scalar('drop_lens/train', i + drop_size, self.interaction_counter)\n",
    "            \n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                self.learn()\n",
    "            if self.interaction_counter % 100 == 0:\n",
    "                self.env.eval()\n",
    "                self.test()\n",
    "                self.env.train()\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        self.epsilon *= 1.005\n",
    "        self.epsilon = min(self.epsilon, 0.98)\n",
    "        \n",
    "        batch_state, batch_reward, batch_next_state, _ = self.memory.sample(self.batch_size)\n",
    "        batch_state  = torch.stack(batch_state, 0).to(self.device)\n",
    "        batch_reward = torch.FloatTensor(batch_reward).view(-1, 1).to(self.device)\n",
    "        batch_next_state = torch.stack(batch_next_state, 0).to(self.device)\n",
    "        \n",
    "        q_eval = self.eval_net_gpu(batch_state)\n",
    "        q_next = self.eval_net_gpu(batch_next_state)\n",
    "        q_target = batch_reward + self.gamma * q_next\n",
    "\n",
    "        loss = F.mse_loss(q_eval, q_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        self.eval_net.load_state_dict(self.eval_net_gpu.state_dict())\n",
    "        self.writer.add_scalar('loss', loss, self.learn_step_counter)\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.eval_net.state_dict(), filename + \"_Q_net\" + str(datetime.datetime.now()))\n",
    "        torch.save(self.optimizer.state_dict(), filename + \"_optimizer_\" + str(datetime.datetime.now()))\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.eval_net.load_state_dict(torch.load(filename + \"_Q_net\"))\n",
    "        self.optimizer.load_state_dict(torch.load(filename + \"_optimizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = ndata[:1000]\n",
    "nlabel = nlabel[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/willer/Desktop/Development/Python/MyRepo/npu-deeplearning-bci/model/PretrainNet_T1.pkl'\n",
    "agent = ADAgent(ndata, nlabel, model_path, train_epoch=8000)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_origin(action):\n",
    "    res = [0 for i in range(10)]\n",
    "    map_move = 0\n",
    "    for act in action:\n",
    "        res[act + map_move] += 1\n",
    "        map_move += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 2, 0, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_to_origin([1,2,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 1])\n",
    "b = np.array([0.1, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
